{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more complex version of this would use this pygithub package as shown in here:\n",
    "- https://python.langchain.com/docs/integrations/toolkits/github\n",
    "\n",
    "But let's try a simpler version that just sets up a bunch of python functions to automate different github actions like add, commit and pull requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a simple commit to a branch of some repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I'll create a github repository using some simple commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# git init -b main\n",
    "\n",
    "# git add . && git commit -m \"Some commit\"\n",
    "\n",
    "# gh repo create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try asking an agent to write come code and commit the resulting code to the current branch of this repository.\n",
    "\n",
    "To do that, let's give the agent the necessary tools it will need which in this case will be python functions that perform different github actions using the `subprocess` package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll follow the basic steps for building a langchain agent:\n",
    "\n",
    "# Steps for building a simple agent:\n",
    "- Set up the LLM\n",
    "- Define the tool or toolkit (list of tools)\n",
    "- Set up a prompt template\n",
    "- Connect llm with your tools\n",
    "- Define your agent as a dict with keys: input, and agent scratchpad \n",
    "- Use the Langchain LCEL language to pipe agent, prompt the llm_with_tools variable and the output parser (which can use OpenAI function calling parser)\n",
    "- Create the agent loop\n",
    "- Wrap everything into the AgentExecutor\n",
    "- Invoke the agent with some query input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the llm\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a tool for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def github_commit_tool(commit_message=\"Some commit\"):\n",
    "    subprocess.run([\"git\", \"add\", \".\"])\n",
    "    subprocess.run([\"git\", \"commit\", \"-m\", commit_message])\n",
    "    subprocess.run([\"git\", \"push\", \"-u\", \"origin\", \"main\"])\n",
    "    \n",
    "    return \"Committed to Github\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now! Before we use it with langchain, let's test it by itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main fd4ef72] Some testing commit\n",
      " 1 file changed, 45 insertions(+), 1 deletion(-)\n",
      "branch 'main' set up to track 'origin/main'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://github.com/EnkrateiaLucca/langchain_github_agent.git\n",
      " * [new branch]      main -> main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Committed to Github'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "github_commit_tool(\"Some testing commit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at out github repository from the terminal.\n",
    "\n",
    "![](2023-10-29-20-47-54.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! It looks like we are good to go with this first simple tool!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now make it a tool for our langchain agent by adding the @tool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def github_commit_tool(commit_message=\"Some commit\"):\n",
    "    \"\"\"This function uses the subprocess package to make commits to a github repo pre-defined.\"\"\"\n",
    "    subprocess.run([\"git\", \"add\", \".\"])\n",
    "    subprocess.run([\"git\", \"commit\", \"-m\", commit_message])\n",
    "    subprocess.run([\"git\", \"push\", \"-u\", \"origin\", \"main\"])\n",
    "    \n",
    "    return \"Committed to Github\"\n",
    "\n",
    "tools = [github_commit_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We added some documentation to our function to abide by the requirements of the tool decorator from langchain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test if a simple agent can use that tool!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by setting up our prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "agent_executor = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to use the github_commit_tool function to make the commit.\n",
      "Action: github_commit_tool\n",
      "Action Input: commit_message='testing commits with langchain agent'\u001b[0m[main 09206f5] commit_message='testing commits with langchain agent'\n",
      " 3 files changed, 457 insertions(+), 8 deletions(-)\n",
      " create mode 100644 2023-10-29-20-47-54.png\n",
      " create mode 100644 basic_langchain_agent.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://github.com/EnkrateiaLucca/langchain_github_agent.git\n",
      "   fd4ef72..09206f5  main -> main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "branch 'main' set up to track 'origin/main'.\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mCommitted to Github\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe commit has been successfully made.\n",
      "Final Answer: The commit has been made to our Github repo with the message 'testing commits with langchain agent'.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"Make a commit to our github repo with the message: 'testing commits with langchain agent'\",\n",
       " 'output': \"The commit has been made to our Github repo with the message 'testing commits with langchain agent'.\"}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"Make a commit to our github repo with the message: 'testing commits with langchain agent'\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And voila! We made a commit!\n",
    "\n",
    "![](2023-10-30-13-04-20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this is cool but, we can go beyond that, and add the ability to create a file and then commit that file to our desired repository. Let's do that!\n",
    "\n",
    "To do it all we have to do is amplify the Agent's toolkit with another tool that can create files in the current folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file_tool(file_contents):\n",
    "    with open(\"file.py\", \"w\") as f:\n",
    "        f.write(file_contents)\n",
    "    \n",
    "    return \"File created\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this function with some contents generated by an llm model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = llm.predict(\"Write Python code for summing two numbers. Output ONLY THE CODE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get the contents from the output of the llm and feed that to the agent along with a file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'File created'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_file_tool(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the file was created and can be run from terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file.py\n"
     ]
    }
   ],
   "source": [
    "!ls *.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "!python ./file.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Now, let's add this to our github agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def create_file_tool(file_contents):\n",
    "    \"\"\"This function creates a file with the contents and file path provided as inputs.\"\"\"\n",
    "    with open(\"file.py\", \"w\") as f:\n",
    "        f.write(file_contents)\n",
    "    \n",
    "    return \"File created\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's add that functionality to our agent by just updating the tools list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [github_commit_tool, create_file_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's modify our agent with its new toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try using both tools in a call to see if the agent can do something slightly more complex now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to use the create_file_tool function to create a file with the Python code for summing two numbers. Then I can use the github_commit_tool function to commit the file to our github repository.\n",
      "Action: create_file_tool\n",
      "Action Input: file_contents = \"def sum_numbers(a, b):\\n    return a + b\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mFile created\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mNow I need to use the github_commit_tool function to commit the file to our github repository.\n",
      "Action: github_commit_tool\n",
      "Action Input: commit_message = \"Added code for summing two numbers\"\u001b[0m[main 02c30b8] commit_message = \"Added code for summing two numbers\n",
      " 3 files changed, 205 insertions(+), 101 deletions(-)\n",
      " create mode 100644 2023-10-30-13-04-20.png\n",
      " create mode 100644 file.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://github.com/EnkrateiaLucca/langchain_github_agent.git\n",
      "   09206f5..02c30b8  main -> main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "branch 'main' set up to track 'origin/main'.\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mCommitted to Github\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: The Python code for summing two numbers has been saved to a file and committed to our github repository.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Write Python code for summing two numbers. Output ONLY THE CODE and save that to a file then commit it to our github repository.',\n",
       " 'output': 'The Python code for summing two numbers has been saved to a file and committed to our github repository.'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\n",
    "    \"input\": \"Write Python code for summing two numbers. Output ONLY THE CODE and save that to a file then commit it to our github repository.\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And voila again!\n",
    "![](2023-10-30-13-15-38.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! This is awesome right? Let's just stop for a second and appreciate the simplicity of this implementation and the power that it gives us! Now, on the other hand, we are using a [ZeroShotReactAgent](https://python.langchain.com/docs/modules/agents/agent_types/react#:~:text=Using-,zeroshotreactagent), which is limited when it comes to multi input functions as tools. However, we can easily expand this by leveraging Langchain's amazing [Multi-Input Tools](https://python.langchain.com/docs/modules/agents/tools/multi_input_tool#:~:text=On%20this%20page-,Multi-Input%20Tools,with%20the%20StructuredTool%20class.,-import%20os).\n",
    "\n",
    "Let's do that to modify our current tool: `create_file_tool()` to accept the file's contents as well as a file path, this way we are not restricted in creating just one type of file with a pre-specified name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'File created'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.tools import StructuredTool\n",
    "\n",
    "def create_file_tool(file_contents: str, file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Function that creates a file with the contents \n",
    "    and file path provided as inputs.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"w\") as f:\n",
    "        f.write(file_contents)\n",
    "    \n",
    "    return \"File created\"\n",
    "\n",
    "# Let's test it first!\n",
    "create_file_tool(\"print('Hello World')\", \"print_file.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file.py       print_file.py\n"
     ]
    }
   ],
   "source": [
    "!ls *.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "!python print_file.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now, let's make it a tool for the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_file_tool = StructuredTool.from_function(create_file_tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, again, let's update our toolkit and our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [github_commit_tool, create_file_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test it by asking it to write a simple `README.md` file, giving the agent some context about the project's repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"create_file_tool\",\n",
      "  \"action_input\": {\n",
      "    \"file_contents\": \"# Introduction to Langchain\\n\\nThis project is an introduction to Langchain, a programming language designed for blockchain development.\\n\",\n",
      "    \"file_path\": \"README.md\"\n",
      "  }\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mFile created\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mNow that the README.md file has been created, I can commit it to the GitHub repository.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"github_commit_tool\",\n",
      "  \"action_input\": {\n",
      "    \"commit_message\": \"Added README.md file\"\n",
      "  }\n",
      "}\n",
      "```\u001b[0m[main f2c61cf] Added README.md file\n",
      " 4 files changed, 214 insertions(+), 5 deletions(-)\n",
      " create mode 100644 2023-10-30-13-15-38.png\n",
      " create mode 100644 README.md\n",
      " create mode 100644 print_file.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://github.com/EnkrateiaLucca/langchain_github_agent.git\n",
      "   02c30b8..f2c61cf  main -> main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "branch 'main' set up to track 'origin/main'.\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mCommitted to Github\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe README.md file has been created and committed to the GitHub repository.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Write a README.md file with a simple description of this project, which is about an introduction to langchain, then commit it to our github repo.',\n",
       " 'output': 'The README.md file has been created and committed to the GitHub repository.'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\n",
    "    \"input\": \"Write a README.md file with a simple description of this project, which is about an introduction to langchain, then commit it to our github repo.\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again we succeed!\n",
    "\n",
    "![](2023-10-30-13-26-32.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, before we finish with this example, let's add a last tool to read files from a repo, for situations where we might write some Python code and we want our agent to integrate that Python code into our repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we follow the same procedure:\n",
    "\n",
    "- Create the tool\n",
    "- Update our tookit\n",
    "- Update our agent\n",
    "- Run a test task to check if it works "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Introduction to Langchain\\n\\nThis project is an introduction to Langchain, a programming language designed for blockchain development.\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_file_tool(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        contents = f.read()\n",
    "    \n",
    "    return contents\n",
    "\n",
    "read_file_tool(\"README.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, our function seems to work so let's add it to our toolkit, since its a simple function with only one input we can just add the @tool decorator on top of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def read_file_tool(file_path):\n",
    "    \"\"\"\n",
    "    Simple function to read contents from a file.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        contents = f.read()\n",
    "    \n",
    "    return contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we update our toolkit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [github_commit_tool, create_file_tool, read_file_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we update our agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = initialize_agent(tools, llm, \n",
    "                                  agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "                                  verbose=True\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we run our newly updated agent, let's create a .txt file containing the contents from this notebook so that our agent has something useful to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"read_file_tool\",\n",
      "  \"action_input\": {\n",
      "    \"file_path\": \"repo_contents.txt\"\n",
      "  }\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mThe more complex version of this would use this pygithub package as shown in here:\n",
      "- https://python.langchain.com/docs/integrations/toolkits/github\n",
      "\n",
      "But let's try a simpler version that just sets up a bunch of python functions to automate different github actions like add, commit and pull requests.\n",
      "Let's start with a simple commit to a branch of some repository.\n",
      "First, I'll create a github repository using some simple commands.\n",
      "# git init -b main\n",
      "\n",
      "# git add . && git commit -m \"Some commit\"\n",
      "\n",
      "# gh repo create\n",
      "Now, let's try asking an agent to write come code and commit the resulting code to the current branch of this repository.\n",
      "\n",
      "To do that, let's give the agent the necessary tools it will need which in this case will be python functions that perform different github actions using the `subprocess` package.\n",
      "We'll follow the basic steps for building a langchain agent:\n",
      "\n",
      "# Steps for building a simple agent:\n",
      "- Set up the LLM\n",
      "- Define the tool or toolkit (list of tools)\n",
      "- Set up a prompt template\n",
      "- Connect llm with your tools\n",
      "- Define your agent as a dict with keys: input, and agent scratchpad \n",
      "- Use the Langchain LCEL language to pipe agent, prompt the llm_with_tools variable and the output parser (which can use OpenAI function calling parser)\n",
      "- Create the agent loop\n",
      "- Wrap everything into the AgentExecutor\n",
      "- Invoke the agent with some query input\n",
      "\n",
      "# setup the llm\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "\n",
      "llm = ChatOpenAI(temperature=0)\n",
      "Now, let's create a tool for the LLM\n",
      "import subprocess\n",
      "\n",
      "def github_commit_tool(commit_message=\"Some commit\"):\n",
      "    subprocess.run([\"git\", \"add\", \".\"])\n",
      "    subprocess.run([\"git\", \"commit\", \"-m\", commit_message])\n",
      "    subprocess.run([\"git\", \"push\", \"-u\", \"origin\", \"main\"])\n",
      "    \n",
      "    return \"Committed to Github\"\n",
      "Now! Before we use it with langchain, let's test it by itself.\n",
      "github_commit_tool(\"Some testing commit\")\n",
      "Now, let's take a look at out github repository from the terminal.\n",
      "\n",
      "![](2023-10-29-20-47-54.png)\n",
      "Nice! It looks like we are good to go with this first simple tool!\n",
      "Let's now make it a tool for our langchain agent by adding the @tool. \n",
      "from langchain.tools import tool\n",
      "\n",
      "@tool\n",
      "def github_commit_tool(commit_message=\"Some commit\"):\n",
      "    \"\"\"This function uses the subprocess package to make commits to a github repo pre-defined.\"\"\"\n",
      "    subprocess.run([\"git\", \"add\", \".\"])\n",
      "    subprocess.run([\"git\", \"commit\", \"-m\", commit_message])\n",
      "    subprocess.run([\"git\", \"push\", \"-u\", \"origin\", \"main\"])\n",
      "    \n",
      "    return \"Committed to Github\"\n",
      "\n",
      "tools = [github_commit_tool]\n",
      "We added some documentation to our function to abide by the requirements of the tool decorator from langchain.\n",
      "Now, let's test if a simple agent can use that tool!\n",
      "Let's start by setting up our prompt template.\n",
      "from langchain.agents import initialize_agent\n",
      "from langchain.agents import AgentType\n",
      "\n",
      "agent_executor = initialize_agent(\n",
      "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
      ")\n",
      "agent_executor.invoke(\n",
      "    {\n",
      "        \"input\": \"Make a commit to our github repo with the message: 'testing commits with langchain agent'\"\n",
      "    }\n",
      ")\n",
      "And voila! We made a commit!\n",
      "\n",
      "![](2023-10-30-13-04-20.png)\n",
      "Ok, this is cool but, we can go beyond that, and add the ability to create a file and then commit that file to our desired repository. Let's do that!\n",
      "\n",
      "To do it all we have to do is amplify the Agent's toolkit with another tool that can create files in the current folder.\n",
      "def create_file_tool(file_contents):\n",
      "    with open(\"file.py\", \"w\") as f:\n",
      "        f.write(file_contents)\n",
      "    \n",
      "    return \"File created\"\n",
      "Let's test this function with some contents generated by an llm model:\n",
      "contents = llm.predict(\"Write Python code for summing two numbers. Output ONLY THE CODE\")\n",
      "Now, let's get the contents from the output of the llm and feed that to the agent along with a file name.\n",
      "create_file_tool(contents)\n",
      "Let's check the file was created and can be run from terminal\n",
      "!ls *.py\n",
      "!python ./file.py\n",
      "Perfect! Now, let's add this to our github agent\n",
      "@tool\n",
      "def create_file_tool(file_contents):\n",
      "    \"\"\"This function creates a file with the contents and file path provided as inputs.\"\"\"\n",
      "    with open(\"file.py\", \"w\") as f:\n",
      "        f.write(file_contents)\n",
      "    \n",
      "    return \"File created\"\n",
      "Now, let's add that functionality to our agent by just updating the tools list.\n",
      "tools = [github_commit_tool, create_file_tool]\n",
      "Now, let's modify our agent with its new toolkit\n",
      "agent_executor = initialize_agent(\n",
      "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
      ")\n",
      "Now, let's try using both tools in a call to see if the agent can do something slightly more complex now.\n",
      "agent_executor.invoke({\n",
      "    \"input\": \"Write Python code for summing two numbers. Output ONLY THE CODE and save that to a file then commit it to our github repository.\"\n",
      "})\n",
      "And voila again!\n",
      "![](2023-10-30-13-15-38.png)\n",
      "Ok! This is awesome right? Let's just stop for a second and appreciate the simplicity of this implementation and the power that it gives us! Now, on the other hand, we are using a [ZeroShotReactAgent](https://python.langchain.com/docs/modules/agents/agent_types/react#:~:text=Using-,zeroshotreactagent), which is limited when it comes to multi input functions as tools. However, we can easily expand this by leveraging Langchain's amazing [Multi-Input Tools](https://python.langchain.com/docs/modules/agents/tools/multi_input_tool#:~:text=On%20this%20page-,Multi-Input%20Tools,with%20the%20StructuredTool%20class.,-import%20os).\n",
      "\n",
      "Let's do that to modify our current tool: `create_file_tool()` to accept the file's contents as well as a file path, this way we are not restricted in creating just one type of file with a pre-specified name.\n",
      "from langchain.tools import StructuredTool\n",
      "\n",
      "def create_file_tool(file_contents: str, file_path: str) -> str:\n",
      "    \"\"\"\n",
      "    Function that creates a file with the contents \n",
      "    and file path provided as inputs.\n",
      "    \"\"\"\n",
      "    with open(file_path, \"w\") as f:\n",
      "        f.write(file_contents)\n",
      "    \n",
      "    return \"File created\"\n",
      "\n",
      "# Let's test it first!\n",
      "create_file_tool(\"print('Hello World')\", \"print_file.py\")\n",
      "!ls *.py\n",
      "!python print_file.py\n",
      "Great! Now, let's make it a tool for the agent.\n",
      "create_file_tool = StructuredTool.from_function(create_file_tool)\n",
      "And, again, let's update our toolkit and our agent.\n",
      "tools = [github_commit_tool, create_file_tool]\n",
      "agent_executor = initialize_agent(\n",
      "    tools,\n",
      "    llm,\n",
      "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
      "    verbose=True,\n",
      ")\n",
      "Now, let's test it by asking it to write a simple `README.md` file, giving the agent some context about the project's repo.\n",
      "agent_executor.invoke({\n",
      "    \"input\": \"Write a README.md file with a simple description of this project, which is about an introduction to langchain, then commit it to our github repo.\"\n",
      "})\n",
      "And again we succeed!\n",
      "\n",
      "![](2023-10-30-13-26-32.png)\n",
      "Ok, before we finish with this example, let's add a last tool to read files from a repo, for situations where we might write some Python code and we want our agent to integrate that Python code into our repository. \n",
      "Again, we follow the same procedure:\n",
      "\n",
      "- Create the tool\n",
      "- Update our tookit\n",
      "- Update our agent\n",
      "- Run a test task to check if it works \n",
      "def read_file_tool(file_path):\n",
      "    with open(file_path, \"r\") as f:\n",
      "        contents = f.read()\n",
      "    \n",
      "    return contents\n",
      "\n",
      "read_file_tool(\"README.md\")\n",
      "Ok, our function seems to work so let's add it to our toolkit, since its a simple function with only one input we can just add the @tool decorator on top of it.\n",
      "@tool\n",
      "def read_file_tool(file_path):\n",
      "    \"\"\"\n",
      "    Simple function to read contents from a file.\n",
      "    \"\"\"\n",
      "    with open(file_path, \"r\") as f:\n",
      "        contents = f.read()\n",
      "    \n",
      "    return contents\n",
      "And we update our toolkit:\n",
      "tools = [github_commit_tool, create_file_tool, read_file_tool]\n",
      "And we update our agent\n",
      "agent_executor = initialize_agent(tools, llm, \n",
      "                                  agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
      "                                  verbose=True\n",
      "                                  )\n",
      "agent_executor.invoke(\n",
      "    {\n",
      "    \"input\": \"Update our README.md file based on the contents of the `langchain_github_agent.ipynb` notebook file, then commit the changes to github repo.\"\n",
      "    }\n",
      "                      )\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe agent has provided a detailed explanation of how to set up a Langchain agent to automate GitHub actions. It starts by setting up the Langchain LLM and defining the necessary tools for GitHub actions, such as committing changes and creating files. The agent then demonstrates how to test and integrate these tools into the Langchain agent. Finally, it shows an example of using the agent to update the README.md file based on the contents of another file and commit the changes to the GitHub repository.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"read_file_tool\",\n",
      "  \"action_input\": {\n",
      "    \"file_path\": \"repo_contents.txt\"\n",
      "  }\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mThe more complex version of this would use this pygithub package as shown in here:\n",
      "- https://python.langchain.com/docs/integrations/toolkits/github\n",
      "\n",
      "But let's try a simpler version that just sets up a bunch of python functions to automate different github actions like add, commit and pull requests.\n",
      "Let's start with a simple commit to a branch of some repository.\n",
      "First, I'll create a github repository using some simple commands.\n",
      "# git init -b main\n",
      "\n",
      "# git add . && git commit -m \"Some commit\"\n",
      "\n",
      "# gh repo create\n",
      "Now, let's try asking an agent to write come code and commit the resulting code to the current branch of this repository.\n",
      "\n",
      "To do that, let's give the agent the necessary tools it will need which in this case will be python functions that perform different github actions using the `subprocess` package.\n",
      "We'll follow the basic steps for building a langchain agent:\n",
      "\n",
      "# Steps for building a simple agent:\n",
      "- Set up the LLM\n",
      "- Define the tool or toolkit (list of tools)\n",
      "- Set up a prompt template\n",
      "- Connect llm with your tools\n",
      "- Define your agent as a dict with keys: input, and agent scratchpad \n",
      "- Use the Langchain LCEL language to pipe agent, prompt the llm_with_tools variable and the output parser (which can use OpenAI function calling parser)\n",
      "- Create the agent loop\n",
      "- Wrap everything into the AgentExecutor\n",
      "- Invoke the agent with some query input\n",
      "\n",
      "# setup the llm\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "\n",
      "llm = ChatOpenAI(temperature=0)\n",
      "Now, let's create a tool for the LLM\n",
      "import subprocess\n",
      "\n",
      "def github_commit_tool(commit_message=\"Some commit\"):\n",
      "    subprocess.run([\"git\", \"add\", \".\"])\n",
      "    subprocess.run([\"git\", \"commit\", \"-m\", commit_message])\n",
      "    subprocess.run([\"git\", \"push\", \"-u\", \"origin\", \"main\"])\n",
      "    \n",
      "    return \"Committed to Github\"\n",
      "Now! Before we use it with langchain, let's test it by itself.\n",
      "github_commit_tool(\"Some testing commit\")\n",
      "Now, let's take a look at out github repository from the terminal.\n",
      "\n",
      "![](2023-10-29-20-47-54.png)\n",
      "Nice! It looks like we are good to go with this first simple tool!\n",
      "Let's now make it a tool for our langchain agent by adding the @tool. \n",
      "from langchain.tools import tool\n",
      "\n",
      "@tool\n",
      "def github_commit_tool(commit_message=\"Some commit\"):\n",
      "    \"\"\"This function uses the subprocess package to make commits to a github repo pre-defined.\"\"\"\n",
      "    subprocess.run([\"git\", \"add\", \".\"])\n",
      "    subprocess.run([\"git\", \"commit\", \"-m\", commit_message])\n",
      "    subprocess.run([\"git\", \"push\", \"-u\", \"origin\", \"main\"])\n",
      "    \n",
      "    return \"Committed to Github\"\n",
      "\n",
      "tools = [github_commit_tool]\n",
      "We added some documentation to our function to abide by the requirements of the tool decorator from langchain.\n",
      "Now, let's test if a simple agent can use that tool!\n",
      "Let's start by setting up our prompt template.\n",
      "from langchain.agents import initialize_agent\n",
      "from langchain.agents import AgentType\n",
      "\n",
      "agent_executor = initialize_agent(\n",
      "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
      ")\n",
      "agent_executor.invoke(\n",
      "    {\n",
      "        \"input\": \"Make a commit to our github repo with the message: 'testing commits with langchain agent'\"\n",
      "    }\n",
      ")\n",
      "And voila! We made a commit!\n",
      "\n",
      "![](2023-10-30-13-04-20.png)\n",
      "Ok, this is cool but, we can go beyond that, and add the ability to create a file and then commit that file to our desired repository. Let's do that!\n",
      "\n",
      "To do it all we have to do is amplify the Agent's toolkit with another tool that can create files in the current folder.\n",
      "def create_file_tool(file_contents):\n",
      "    with open(\"file.py\", \"w\") as f:\n",
      "        f.write(file_contents)\n",
      "    \n",
      "    return \"File created\"\n",
      "Let's test this function with some contents generated by an llm model:\n",
      "contents = llm.predict(\"Write Python code for summing two numbers. Output ONLY THE CODE\")\n",
      "Now, let's get the contents from the output of the llm and feed that to the agent along with a file name.\n",
      "create_file_tool(contents)\n",
      "Let's check the file was created and can be run from terminal\n",
      "!ls *.py\n",
      "!python ./file.py\n",
      "Perfect! Now, let's add this to our github agent\n",
      "@tool\n",
      "def create_file_tool(file_contents):\n",
      "    \"\"\"This function creates a file with the contents and file path provided as inputs.\"\"\"\n",
      "    with open(\"file.py\", \"w\") as f:\n",
      "        f.write(file_contents)\n",
      "    \n",
      "    return \"File created\"\n",
      "Now, let's add that functionality to our agent by just updating the tools list.\n",
      "tools = [github_commit_tool, create_file_tool]\n",
      "Now, let's modify our agent with its new toolkit\n",
      "agent_executor = initialize_agent(\n",
      "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
      ")\n",
      "Now, let's try using both tools in a call to see if the agent can do something slightly more complex now.\n",
      "agent_executor.invoke({\n",
      "    \"input\": \"Write Python code for summing two numbers. Output ONLY THE CODE and save that to a file then commit it to our github repository.\"\n",
      "})\n",
      "And voila again!\n",
      "![](2023-10-30-13-15-38.png)\n",
      "Ok! This is awesome right? Let's just stop for a second and appreciate the simplicity of this implementation and the power that it gives us! Now, on the other hand, we are using a [ZeroShotReactAgent](https://python.langchain.com/docs/modules/agents/agent_types/react#:~:text=Using-,zeroshotreactagent), which is limited when it comes to multi input functions as tools. However, we can easily expand this by leveraging Langchain's amazing [Multi-Input Tools](https://python.langchain.com/docs/modules/agents/tools/multi_input_tool#:~:text=On%20this%20page-,Multi-Input%20Tools,with%20the%20StructuredTool%20class.,-import%20os).\n",
      "\n",
      "Let's do that to modify our current tool: `create_file_tool()` to accept the file's contents as well as a file path, this way we are not restricted in creating just one type of file with a pre-specified name.\n",
      "from langchain.tools import StructuredTool\n",
      "\n",
      "def create_file_tool(file_contents: str, file_path: str) -> str:\n",
      "    \"\"\"\n",
      "    Function that creates a file with the contents \n",
      "    and file path provided as inputs.\n",
      "    \"\"\"\n",
      "    with open(file_path, \"w\") as f:\n",
      "        f.write(file_contents)\n",
      "    \n",
      "    return \"File created\"\n",
      "\n",
      "# Let's test it first!\n",
      "create_file_tool(\"print('Hello World')\", \"print_file.py\")\n",
      "!ls *.py\n",
      "!python print_file.py\n",
      "Great! Now, let's make it a tool for the agent.\n",
      "create_file_tool = StructuredTool.from_function(create_file_tool)\n",
      "And, again, let's update our toolkit and our agent.\n",
      "tools = [github_commit_tool, create_file_tool]\n",
      "agent_executor = initialize_agent(\n",
      "    tools,\n",
      "    llm,\n",
      "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
      "    verbose=True,\n",
      ")\n",
      "Now, let's test it by asking it to write a simple `README.md` file, giving the agent some context about the project's repo.\n",
      "agent_executor.invoke({\n",
      "    \"input\": \"Write a README.md file with a simple description of this project, which is about an introduction to langchain, then commit it to our github repo.\"\n",
      "})\n",
      "And again we succeed!\n",
      "\n",
      "![](2023-10-30-13-26-32.png)\n",
      "Ok, before we finish with this example, let's add a last tool to read files from a repo, for situations where we might write some Python code and we want our agent to integrate that Python code into our repository. \n",
      "Again, we follow the same procedure:\n",
      "\n",
      "- Create the tool\n",
      "- Update our tookit\n",
      "- Update our agent\n",
      "- Run a test task to check if it works \n",
      "def read_file_tool(file_path):\n",
      "    with open(file_path, \"r\") as f:\n",
      "        contents = f.read()\n",
      "    \n",
      "    return contents\n",
      "\n",
      "read_file_tool(\"README.md\")\n",
      "Ok, our function seems to work so let's add it to our toolkit, since its a simple function with only one input we can just add the @tool decorator on top of it.\n",
      "@tool\n",
      "def read_file_tool(file_path):\n",
      "    \"\"\"\n",
      "    Simple function to read contents from a file.\n",
      "    \"\"\"\n",
      "    with open(file_path, \"r\") as f:\n",
      "        contents = f.read()\n",
      "    \n",
      "    return contents\n",
      "And we update our toolkit:\n",
      "tools = [github_commit_tool, create_file_tool, read_file_tool]\n",
      "And we update our agent\n",
      "agent_executor = initialize_agent(tools, llm, \n",
      "                                  agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
      "                                  verbose=True\n",
      "                                  )\n",
      "agent_executor.invoke(\n",
      "    {\n",
      "    \"input\": \"Update our README.md file based on the contents of the `langchain_github_agent.ipynb` notebook file, then commit the changes to github repo.\"\n",
      "    }\n",
      "                      )\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 4097 tokens. However, your messages resulted in 4517 tokens. Please reduce the length of the messages.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/greatmaster/Desktop/projects/content_creation/langchain_github_agent/langchain_github_agent.ipynb Cell 62\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/content_creation/langchain_github_agent/langchain_github_agent.ipynb#Y211sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m agent_executor\u001b[39m.\u001b[39;49minvoke(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/content_creation/langchain_github_agent/langchain_github_agent.ipynb#Y211sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     {\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/content_creation/langchain_github_agent/langchain_github_agent.ipynb#Y211sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39minput\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mUpdate our README.md file based on the contents of the `repo_contents.txt` file, then commit the changes to github repo.\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/content_creation/langchain_github_agent/langchain_github_agent.ipynb#Y211sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     }\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/content_creation/langchain_github_agent/langchain_github_agent.ipynb#Y211sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                       )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/chains/base.py:87\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[1;32m     81\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     82\u001b[0m     \u001b[39minput\u001b[39m: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[1;32m     83\u001b[0m     config: Optional[RunnableConfig] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     84\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m     85\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[1;32m     86\u001b[0m     config \u001b[39m=\u001b[39m config \u001b[39mor\u001b[39;00m {}\n\u001b[0;32m---> 87\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m     88\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[1;32m     89\u001b[0m         callbacks\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcallbacks\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m     90\u001b[0m         tags\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mtags\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m     91\u001b[0m         metadata\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m     92\u001b[0m         run_name\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mrun_name\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m     93\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     94\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/chains/base.py:310\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 310\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    311\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    312\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    313\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    314\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/chains/base.py:304\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    297\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    298\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    299\u001b[0m     inputs,\n\u001b[1;32m    300\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[1;32m    301\u001b[0m )\n\u001b[1;32m    302\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 304\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    305\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    306\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    307\u001b[0m     )\n\u001b[1;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/agents/agent.py:1146\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[39m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m-> 1146\u001b[0m     next_step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take_next_step(\n\u001b[1;32m   1147\u001b[0m         name_to_tool_map,\n\u001b[1;32m   1148\u001b[0m         color_mapping,\n\u001b[1;32m   1149\u001b[0m         inputs,\n\u001b[1;32m   1150\u001b[0m         intermediate_steps,\n\u001b[1;32m   1151\u001b[0m         run_manager\u001b[39m=\u001b[39;49mrun_manager,\n\u001b[1;32m   1152\u001b[0m     )\n\u001b[1;32m   1153\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m   1154\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return(\n\u001b[1;32m   1155\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[39m=\u001b[39mrun_manager\n\u001b[1;32m   1156\u001b[0m         )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/agents/agent.py:933\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m    930\u001b[0m     intermediate_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_intermediate_steps(intermediate_steps)\n\u001b[1;32m    932\u001b[0m     \u001b[39m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magent\u001b[39m.\u001b[39;49mplan(\n\u001b[1;32m    934\u001b[0m         intermediate_steps,\n\u001b[1;32m    935\u001b[0m         callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    936\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs,\n\u001b[1;32m    937\u001b[0m     )\n\u001b[1;32m    938\u001b[0m \u001b[39mexcept\u001b[39;00m OutputParserException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    939\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_parsing_errors, \u001b[39mbool\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/agents/agent.py:546\u001b[0m, in \u001b[0;36mAgent.plan\u001b[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Given input, decided what to do.\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \n\u001b[1;32m    536\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[39m    Action specifying what tool to use.\u001b[39;00m\n\u001b[1;32m    544\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    545\u001b[0m full_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_full_inputs(intermediate_steps, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 546\u001b[0m full_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_chain\u001b[39m.\u001b[39;49mpredict(callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfull_inputs)\n\u001b[1;32m    547\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_parser\u001b[39m.\u001b[39mparse(full_output)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/chains/llm.py:257\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, callbacks: Callbacks \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    243\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[1;32m    245\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/chains/base.py:310\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 310\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    311\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    312\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    313\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    314\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/chains/base.py:304\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    297\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    298\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    299\u001b[0m     inputs,\n\u001b[1;32m    300\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[1;32m    301\u001b[0m )\n\u001b[1;32m    302\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 304\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    305\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    306\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    307\u001b[0m     )\n\u001b[1;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/chains/llm.py:93\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[1;32m     89\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     90\u001b[0m     inputs: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[1;32m     91\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     92\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m---> 93\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate([inputs], run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m     94\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/chains/llm.py:103\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m prompts, stop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_prompts(input_list, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[1;32m    104\u001b[0m     prompts,\n\u001b[1;32m    105\u001b[0m     stop,\n\u001b[1;32m    106\u001b[0m     callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    107\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_kwargs,\n\u001b[1;32m    108\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/chat_models/base.py:459\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    452\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    453\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    457\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    458\u001b[0m     prompt_messages \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_messages() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 459\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_messages, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/chat_models/base.py:349\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n\u001b[1;32m    348\u001b[0m             run_managers[i]\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 349\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    350\u001b[0m flattened_outputs \u001b[39m=\u001b[39m [\n\u001b[1;32m    351\u001b[0m     LLMResult(generations\u001b[39m=\u001b[39m[res\u001b[39m.\u001b[39mgenerations], llm_output\u001b[39m=\u001b[39mres\u001b[39m.\u001b[39mllm_output)\n\u001b[1;32m    352\u001b[0m     \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results\n\u001b[1;32m    353\u001b[0m ]\n\u001b[1;32m    354\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/chat_models/base.py:339\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[39mfor\u001b[39;00m i, m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(messages):\n\u001b[1;32m    337\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m         results\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 339\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_with_cache(\n\u001b[1;32m    340\u001b[0m                 m,\n\u001b[1;32m    341\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    342\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[i] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    343\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    344\u001b[0m             )\n\u001b[1;32m    345\u001b[0m         )\n\u001b[1;32m    346\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    347\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/chat_models/base.py:492\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    489\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    490\u001b[0m     )\n\u001b[1;32m    491\u001b[0m \u001b[39mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 492\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    493\u001b[0m         messages, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    494\u001b[0m     )\n\u001b[1;32m    495\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(messages, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/chat_models/openai.py:360\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m message_dicts, params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    359\u001b[0m params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs}\n\u001b[0;32m--> 360\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompletion_with_retry(\n\u001b[1;32m    361\u001b[0m     messages\u001b[39m=\u001b[39;49mmessage_dicts, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[1;32m    362\u001b[0m )\n\u001b[1;32m    363\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/chat_models/openai.py:299\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    296\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 299\u001b[0m \u001b[39mreturn\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[1;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/tenacity/__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_explicit_retry \u001b[39m=\u001b[39m fut\u001b[39m.\u001b[39mfailed \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(fut\u001b[39m.\u001b[39mexception(), TryAgain)\n\u001b[1;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_explicit_retry \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry(retry_state)):\n\u001b[0;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter(retry_state)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    451\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    383\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain/chat_models/openai.py:297\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry.<locals>._completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    296\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:155\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    131\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    139\u001b[0m ):\n\u001b[1;32m    140\u001b[0m     (\n\u001b[1;32m    141\u001b[0m         deployment_id,\n\u001b[1;32m    142\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    153\u001b[0m     )\n\u001b[0;32m--> 155\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    156\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    157\u001b[0m         url,\n\u001b[1;32m    158\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    159\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    160\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    161\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    162\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    166\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    167\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/openai/api_requestor.py:299\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    279\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    280\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    288\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    289\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    290\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    291\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    297\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    298\u001b[0m     )\n\u001b[0;32m--> 299\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    300\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/openai/api_requestor.py:710\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    703\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    704\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    707\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    709\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 710\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    711\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    712\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    713\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    714\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    715\u001b[0m         ),\n\u001b[1;32m    716\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    717\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/openai/api_requestor.py:775\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    773\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    774\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 775\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    776\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    777\u001b[0m     )\n\u001b[1;32m    778\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: This model's maximum context length is 4097 tokens. However, your messages resulted in 4517 tokens. Please reduce the length of the messages."
     ]
    }
   ],
   "source": [
    "agent_executor.invoke(\n",
    "    {\n",
    "    \"input\": \"Update our README.md file based on the contents of the `repo_contents.txt` file, then commit the changes to github repo.\"\n",
    "    }\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly_agents",
   "language": "python",
   "name": "oreilly_agents"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
